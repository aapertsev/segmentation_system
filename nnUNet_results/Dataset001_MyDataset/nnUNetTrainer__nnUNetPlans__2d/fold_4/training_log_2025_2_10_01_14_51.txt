
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-02-10 01:14:53.059194: do_dummy_2d_data_aug: False 
2025-02-10 01:14:53.060367: Using splits from existing split file: /Users/aleksandr/Desktop/nnU-net/nnUNet_preprocessed/Dataset001_MyDataset/splits_final.json 
2025-02-10 01:14:53.060874: The split file contains 5 splits. 
2025-02-10 01:14:53.060924: Desired fold for training: 4 
2025-02-10 01:14:53.060970: This split has 138 training and 34 validation cases. 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 8, 'patch_size': [448, 512], 'median_image_size_in_voxels': [408.0, 512.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset001_MyDataset', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [999.0, 1.0, 1.0], 'original_median_shape_after_transp': [1, 408, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 255.0, 'mean': 107.11954498291016, 'median': 110.0, 'min': 0.0, 'percentile_00_5': 0.0, 'percentile_99_5': 162.0, 'std': 25.942211151123047}}} 
 
2025-02-10 01:15:06.971221: unpacking dataset... 
2025-02-10 01:15:11.108336: unpacking done... 
2025-02-10 01:15:14.408532: Unable to plot network architecture: 
2025-02-10 01:15:14.408944: module 'torch.onnx' has no attribute '_optimize_graph' 
2025-02-10 01:15:14.453695:  
2025-02-10 01:15:14.453820: Epoch 50 
2025-02-10 01:15:14.454042: Current learning rate: 0.00955 
2025-02-10 01:22:06.576015: train_loss -0.9199 
2025-02-10 01:22:06.579176: val_loss -0.8589 
2025-02-10 01:22:06.579269: Pseudo dice [0.9511, 0.9701, 0.804] 
2025-02-10 01:22:06.580168: Epoch time: 412.12 s 
2025-02-10 01:22:07.905240:  
2025-02-10 01:22:07.905741: Epoch 51 
2025-02-10 01:22:07.905847: Current learning rate: 0.00954 
2025-02-10 01:28:36.504445: train_loss -0.9119 
2025-02-10 01:28:36.506282: val_loss -0.8783 
2025-02-10 01:28:36.506413: Pseudo dice [0.9444, 0.9728, 0.8372] 
2025-02-10 01:28:36.507142: Epoch time: 388.6 s 
2025-02-10 01:28:36.507309: Yayy! New best EMA pseudo Dice: 0.911 
2025-02-10 01:28:38.421216:  
2025-02-10 01:28:38.422655: Epoch 52 
2025-02-10 01:28:38.422866: Current learning rate: 0.00953 
2025-02-10 01:35:07.930240: train_loss -0.9114 
2025-02-10 01:35:07.932788: val_loss -0.8659 
2025-02-10 01:35:07.932934: Pseudo dice [0.9462, 0.9717, 0.8121] 
2025-02-10 01:35:07.933973: Epoch time: 389.51 s 
2025-02-10 01:35:08.847977:  
2025-02-10 01:35:08.848425: Epoch 53 
2025-02-10 01:35:08.848541: Current learning rate: 0.00952 
2025-02-10 01:41:38.982542: train_loss -0.9202 
2025-02-10 01:41:38.985393: val_loss -0.8933 
2025-02-10 01:41:38.985830: Pseudo dice [0.9477, 0.9747, 0.8694] 
2025-02-10 01:41:38.986974: Epoch time: 390.14 s 
2025-02-10 01:41:38.987327: Yayy! New best EMA pseudo Dice: 0.9128 
2025-02-10 01:41:40.282388:  
2025-02-10 01:41:40.282736: Epoch 54 
2025-02-10 01:41:40.282833: Current learning rate: 0.00951 
2025-02-10 01:48:09.405650: train_loss -0.9234 
2025-02-10 01:48:09.408196: val_loss -0.8866 
2025-02-10 01:48:09.408344: Pseudo dice [0.9503, 0.9765, 0.8701] 
2025-02-10 01:48:09.409262: Epoch time: 389.12 s 
2025-02-10 01:48:09.409490: Yayy! New best EMA pseudo Dice: 0.9148 
2025-02-10 01:48:10.652843:  
2025-02-10 01:48:10.653206: Epoch 55 
2025-02-10 01:48:10.653307: Current learning rate: 0.0095 
2025-02-10 01:54:39.795708: train_loss -0.9238 
2025-02-10 01:54:39.798064: val_loss -0.8719 
2025-02-10 01:54:39.798226: Pseudo dice [0.9378, 0.9755, 0.8589] 
2025-02-10 01:54:39.798852: Epoch time: 389.14 s 
2025-02-10 01:54:39.798989: Yayy! New best EMA pseudo Dice: 0.9157 
2025-02-10 01:54:41.033119:  
2025-02-10 01:54:41.033469: Epoch 56 
2025-02-10 01:54:41.033584: Current learning rate: 0.00949 
2025-02-10 02:01:10.984763: train_loss -0.9266 
2025-02-10 02:01:10.987356: val_loss -0.8991 
2025-02-10 02:01:10.987473: Pseudo dice [0.9505, 0.9736, 0.8569] 
2025-02-10 02:01:10.988227: Epoch time: 389.95 s 
2025-02-10 02:01:10.988318: Yayy! New best EMA pseudo Dice: 0.9168 
2025-02-10 02:01:12.284007:  
2025-02-10 02:01:12.284345: Epoch 57 
2025-02-10 02:01:12.284479: Current learning rate: 0.00949 
2025-02-10 02:07:42.368183: train_loss -0.9114 
2025-02-10 02:07:42.370579: val_loss -0.8558 
2025-02-10 02:07:42.370730: Pseudo dice [0.9429, 0.9734, 0.8374] 
2025-02-10 02:07:42.371400: Epoch time: 390.08 s 
2025-02-10 02:07:42.371541: Yayy! New best EMA pseudo Dice: 0.9169 
2025-02-10 02:07:43.602032:  
2025-02-10 02:07:43.602380: Epoch 58 
2025-02-10 02:07:43.602504: Current learning rate: 0.00948 
2025-02-10 02:14:14.049061: train_loss -0.9144 
2025-02-10 02:14:14.051710: val_loss -0.8588 
2025-02-10 02:14:14.052217: Pseudo dice [0.9466, 0.9719, 0.8058] 
2025-02-10 02:14:14.053347: Epoch time: 390.45 s 
2025-02-10 02:14:14.945567:  
2025-02-10 02:14:14.945740: Epoch 59 
2025-02-10 02:14:14.945834: Current learning rate: 0.00947 
2025-02-10 02:20:44.526511: train_loss -0.9158 
2025-02-10 02:20:44.528866: val_loss -0.8701 
2025-02-10 02:20:44.529028: Pseudo dice [0.9453, 0.9731, 0.8454] 
2025-02-10 02:20:44.529759: Epoch time: 389.58 s 
2025-02-10 02:20:45.402726:  
2025-02-10 02:20:45.402933: Epoch 60 
2025-02-10 02:20:45.403034: Current learning rate: 0.00946 
2025-02-10 02:27:15.524446: train_loss -0.9235 
2025-02-10 02:27:15.526414: val_loss -0.875 
2025-02-10 02:27:15.526509: Pseudo dice [0.9487, 0.9747, 0.8608] 
2025-02-10 02:27:15.527050: Epoch time: 390.12 s 
2025-02-10 02:27:15.527112: Yayy! New best EMA pseudo Dice: 0.9177 
2025-02-10 02:27:17.730144:  
2025-02-10 02:27:17.730364: Epoch 61 
2025-02-10 02:27:17.730461: Current learning rate: 0.00945 
2025-02-10 02:33:48.712925: train_loss -0.9265 
2025-02-10 02:33:48.715715: val_loss -0.8822 
2025-02-10 02:33:48.716250: Pseudo dice [0.9513, 0.9735, 0.8133] 
2025-02-10 02:33:48.717122: Epoch time: 390.98 s 
2025-02-10 02:33:49.639328:  
2025-02-10 02:33:49.639692: Epoch 62 
2025-02-10 02:33:49.639798: Current learning rate: 0.00944 
2025-02-10 02:40:19.874258: train_loss -0.9198 
2025-02-10 02:40:19.876673: val_loss -0.8798 
2025-02-10 02:40:19.876769: Pseudo dice [0.9498, 0.9735, 0.8526] 
2025-02-10 02:40:19.878101: Epoch time: 390.24 s 
2025-02-10 02:40:19.878219: Yayy! New best EMA pseudo Dice: 0.918 
2025-02-10 02:40:21.209817:  
2025-02-10 02:40:21.210229: Epoch 63 
2025-02-10 02:40:21.210339: Current learning rate: 0.00943 
2025-02-10 02:46:50.347853: train_loss -0.9226 
2025-02-10 02:46:50.350232: val_loss -0.8702 
2025-02-10 02:46:50.350375: Pseudo dice [0.9393, 0.9761, 0.8294] 
2025-02-10 02:46:50.351063: Epoch time: 389.14 s 
2025-02-10 02:46:51.203517:  
2025-02-10 02:46:51.203708: Epoch 64 
2025-02-10 02:46:51.203812: Current learning rate: 0.00942 
2025-02-10 02:53:18.599225: train_loss -0.9207 
2025-02-10 02:53:18.601585: val_loss -0.8793 
2025-02-10 02:53:18.601722: Pseudo dice [0.9504, 0.9727, 0.8301] 
2025-02-10 02:53:18.602402: Epoch time: 387.4 s 
2025-02-10 02:53:19.458724:  
2025-02-10 02:53:19.458933: Epoch 65 
2025-02-10 02:53:19.459042: Current learning rate: 0.00941 
2025-02-10 02:59:46.664983: train_loss -0.9307 
2025-02-10 02:59:46.667568: val_loss -0.8879 
2025-02-10 02:59:46.667719: Pseudo dice [0.9447, 0.9736, 0.8381] 
2025-02-10 02:59:46.668493: Epoch time: 387.21 s 
2025-02-10 02:59:47.527025:  
2025-02-10 02:59:47.527221: Epoch 66 
2025-02-10 02:59:47.527323: Current learning rate: 0.0094 
2025-02-10 03:06:17.108633: train_loss -0.9226 
2025-02-10 03:06:17.111591: val_loss -0.8782 
2025-02-10 03:06:17.111747: Pseudo dice [0.9475, 0.9715, 0.8307] 
2025-02-10 03:06:17.113103: Epoch time: 389.58 s 
2025-02-10 03:06:18.032337:  
2025-02-10 03:06:18.032726: Epoch 67 
2025-02-10 03:06:18.032828: Current learning rate: 0.00939 
2025-02-10 03:12:49.474104: train_loss -0.9267 
2025-02-10 03:12:49.477189: val_loss -0.8798 
2025-02-10 03:12:49.477334: Pseudo dice [0.948, 0.9722, 0.8471] 
2025-02-10 03:12:49.478217: Epoch time: 391.44 s 
2025-02-10 03:12:49.478373: Yayy! New best EMA pseudo Dice: 0.9182 
2025-02-10 03:12:50.792311:  
2025-02-10 03:12:50.792725: Epoch 68 
2025-02-10 03:12:50.792920: Current learning rate: 0.00939 
2025-02-10 03:19:20.568119: train_loss -0.9294 
2025-02-10 03:19:20.571439: val_loss -0.8882 
2025-02-10 03:19:20.571734: Pseudo dice [0.943, 0.9746, 0.8666] 
2025-02-10 03:19:20.572629: Epoch time: 389.78 s 
2025-02-10 03:19:20.572800: Yayy! New best EMA pseudo Dice: 0.9192 
2025-02-10 03:19:21.967147:  
2025-02-10 03:19:21.967377: Epoch 69 
2025-02-10 03:19:21.967488: Current learning rate: 0.00938 
2025-02-10 03:25:52.506112: train_loss -0.922 
2025-02-10 03:25:52.509558: val_loss -0.8777 
2025-02-10 03:25:52.509710: Pseudo dice [0.9349, 0.9755, 0.8617] 
2025-02-10 03:25:52.510452: Epoch time: 390.54 s 
2025-02-10 03:25:52.510544: Yayy! New best EMA pseudo Dice: 0.9197 
2025-02-10 03:25:54.752532:  
2025-02-10 03:25:54.752730: Epoch 70 
2025-02-10 03:25:54.752832: Current learning rate: 0.00937 
2025-02-10 03:32:25.374429: train_loss -0.9192 
2025-02-10 03:32:25.377984: val_loss -0.8431 
2025-02-10 03:32:25.378161: Pseudo dice [0.9353, 0.9722, 0.8363] 
2025-02-10 03:32:25.379174: Epoch time: 390.62 s 
2025-02-10 03:32:26.343202:  
2025-02-10 03:32:26.343597: Epoch 71 
2025-02-10 03:32:26.343697: Current learning rate: 0.00936 
2025-02-10 03:38:56.366985: train_loss -0.9024 
2025-02-10 03:38:56.369991: val_loss -0.8834 
2025-02-10 03:38:56.370168: Pseudo dice [0.9527, 0.9708, 0.8548] 
2025-02-10 03:38:56.370904: Epoch time: 390.02 s 
2025-02-10 03:38:56.371085: Yayy! New best EMA pseudo Dice: 0.9198 
2025-02-10 03:38:57.693769:  
2025-02-10 03:38:57.694165: Epoch 72 
2025-02-10 03:38:57.694283: Current learning rate: 0.00935 
2025-02-10 03:45:27.255856: train_loss -0.9125 
2025-02-10 03:45:27.257914: val_loss -0.8615 
2025-02-10 03:45:27.258013: Pseudo dice [0.9421, 0.9717, 0.8239] 
2025-02-10 03:45:27.258606: Epoch time: 389.56 s 
2025-02-10 03:45:28.169553:  
2025-02-10 03:45:28.169803: Epoch 73 
2025-02-10 03:45:28.169925: Current learning rate: 0.00934 
2025-02-10 03:51:58.494372: train_loss -0.9132 
2025-02-10 03:51:58.496828: val_loss -0.8858 
2025-02-10 03:51:58.496990: Pseudo dice [0.9467, 0.9732, 0.8575] 
2025-02-10 03:51:58.497613: Epoch time: 390.33 s 
2025-02-10 03:51:59.370296:  
2025-02-10 03:51:59.370496: Epoch 74 
2025-02-10 03:51:59.370664: Current learning rate: 0.00933 
2025-02-10 03:58:28.597691: train_loss -0.9144 
2025-02-10 03:58:28.600083: val_loss -0.855 
2025-02-10 03:58:28.600246: Pseudo dice [0.9414, 0.9716, 0.8146] 
2025-02-10 03:58:28.600970: Epoch time: 389.23 s 
2025-02-10 03:58:29.507967:  
2025-02-10 03:58:29.508212: Epoch 75 
2025-02-10 03:58:29.508323: Current learning rate: 0.00932 
2025-02-10 04:04:58.917325: train_loss -0.9195 
2025-02-10 04:04:58.920255: val_loss -0.8719 
2025-02-10 04:04:58.920439: Pseudo dice [0.9415, 0.9729, 0.8521] 
2025-02-10 04:04:58.921151: Epoch time: 389.41 s 
2025-02-10 04:04:59.802486:  
2025-02-10 04:04:59.802724: Epoch 76 
2025-02-10 04:04:59.802831: Current learning rate: 0.00931 
2025-02-10 04:11:29.638554: train_loss -0.92 
2025-02-10 04:11:29.641031: val_loss -0.8849 
2025-02-10 04:11:29.641192: Pseudo dice [0.9343, 0.9722, 0.8679] 
2025-02-10 04:11:29.641899: Epoch time: 389.84 s 
2025-02-10 04:11:30.563879:  
2025-02-10 04:11:30.564085: Epoch 77 
2025-02-10 04:11:30.564196: Current learning rate: 0.0093 
2025-02-10 04:18:01.579559: train_loss -0.9249 
2025-02-10 04:18:01.582427: val_loss -0.856 
2025-02-10 04:18:01.582885: Pseudo dice [0.9468, 0.9744, 0.7951] 
2025-02-10 04:18:01.584224: Epoch time: 391.02 s 
2025-02-10 04:18:03.438718:  
2025-02-10 04:18:03.438955: Epoch 78 
2025-02-10 04:18:03.439064: Current learning rate: 0.0093 
2025-02-10 04:24:34.530975: train_loss -0.918 
2025-02-10 04:24:34.534059: val_loss -0.8582 
2025-02-10 04:24:34.534492: Pseudo dice [0.9374, 0.9735, 0.859] 
2025-02-10 04:24:34.535359: Epoch time: 391.09 s 
2025-02-10 04:24:35.446714:  
2025-02-10 04:24:35.447213: Epoch 79 
2025-02-10 04:24:35.447333: Current learning rate: 0.00929 
2025-02-10 04:31:05.885542: train_loss -0.9247 
2025-02-10 04:31:05.888854: val_loss -0.881 
2025-02-10 04:31:05.889103: Pseudo dice [0.9466, 0.9744, 0.8625] 
2025-02-10 04:31:05.890208: Epoch time: 390.44 s 
2025-02-10 04:31:06.824140:  
2025-02-10 04:31:06.824544: Epoch 80 
2025-02-10 04:31:06.824652: Current learning rate: 0.00928 
2025-02-10 04:37:38.022450: train_loss -0.9216 
2025-02-10 04:37:38.025625: val_loss -0.8846 
2025-02-10 04:37:38.025758: Pseudo dice [0.9372, 0.9716, 0.8676] 
2025-02-10 04:37:38.026786: Epoch time: 391.2 s 
2025-02-10 04:37:38.027224: Yayy! New best EMA pseudo Dice: 0.9202 
2025-02-10 04:37:39.455404:  
2025-02-10 04:37:39.455817: Epoch 81 
2025-02-10 04:37:39.455935: Current learning rate: 0.00927 
2025-02-10 04:44:09.753867: train_loss -0.9274 
2025-02-10 04:44:09.757106: val_loss -0.8733 
2025-02-10 04:44:09.757285: Pseudo dice [0.9414, 0.9742, 0.8317] 
2025-02-10 04:44:09.757990: Epoch time: 390.3 s 
2025-02-10 04:44:10.690941:  
2025-02-10 04:44:10.691143: Epoch 82 
2025-02-10 04:44:10.691254: Current learning rate: 0.00926 
2025-02-10 04:50:40.885978: train_loss -0.9312 
2025-02-10 04:50:40.889222: val_loss -0.8679 
2025-02-10 04:50:40.889578: Pseudo dice [0.9407, 0.9736, 0.8246] 
2025-02-10 04:50:40.890340: Epoch time: 390.2 s 
2025-02-10 04:50:41.789326:  
2025-02-10 04:50:41.789935: Epoch 83 
2025-02-10 04:50:41.790038: Current learning rate: 0.00925 
2025-02-10 04:57:12.191400: train_loss -0.9261 
2025-02-10 04:57:12.194477: val_loss -0.8561 
2025-02-10 04:57:12.195040: Pseudo dice [0.9418, 0.9755, 0.8229] 
2025-02-10 04:57:12.196036: Epoch time: 390.4 s 
2025-02-10 04:57:13.088550:  
2025-02-10 04:57:13.088743: Epoch 84 
2025-02-10 04:57:13.088866: Current learning rate: 0.00924 
2025-02-10 05:03:43.145855: train_loss -0.9268 
2025-02-10 05:03:43.148601: val_loss -0.8854 
2025-02-10 05:03:43.148741: Pseudo dice [0.9351, 0.9763, 0.8732] 
2025-02-10 05:03:43.149595: Epoch time: 390.06 s 
2025-02-10 05:03:43.996498:  
2025-02-10 05:03:43.996686: Epoch 85 
2025-02-10 05:03:43.996790: Current learning rate: 0.00923 
2025-02-10 05:10:14.232451: train_loss -0.9321 
2025-02-10 05:10:14.234952: val_loss -0.8773 
2025-02-10 05:10:14.235121: Pseudo dice [0.9455, 0.97, 0.8238] 
2025-02-10 05:10:14.235909: Epoch time: 390.24 s 
2025-02-10 05:10:16.126797:  
2025-02-10 05:10:16.126993: Epoch 86 
2025-02-10 05:10:16.127131: Current learning rate: 0.00922 
2025-02-10 05:16:46.936944: train_loss -0.9302 
2025-02-10 05:16:46.940170: val_loss -0.8678 
2025-02-10 05:16:46.940329: Pseudo dice [0.9405, 0.9721, 0.8271] 
2025-02-10 05:16:46.941018: Epoch time: 390.81 s 
2025-02-10 05:16:47.802188:  
2025-02-10 05:16:47.802394: Epoch 87 
2025-02-10 05:16:47.802490: Current learning rate: 0.00921 
2025-02-10 05:23:17.330502: train_loss -0.9279 
2025-02-10 05:23:17.333986: val_loss -0.8663 
2025-02-10 05:23:17.334135: Pseudo dice [0.9366, 0.9741, 0.8264] 
2025-02-10 05:23:17.334894: Epoch time: 389.53 s 
2025-02-10 05:23:18.227898:  
2025-02-10 05:23:18.228118: Epoch 88 
2025-02-10 05:23:18.228215: Current learning rate: 0.0092 
2025-02-10 05:29:48.947043: train_loss -0.931 
2025-02-10 05:29:48.949229: val_loss -0.8885 
2025-02-10 05:29:48.949393: Pseudo dice [0.9401, 0.975, 0.8567] 
2025-02-10 05:29:48.950266: Epoch time: 390.72 s 
2025-02-10 05:29:49.830943:  
2025-02-10 05:29:49.831155: Epoch 89 
2025-02-10 05:29:49.831263: Current learning rate: 0.0092 
2025-02-10 05:36:20.606193: train_loss -0.9248 
2025-02-10 05:36:20.609143: val_loss -0.8756 
2025-02-10 05:36:20.609267: Pseudo dice [0.9468, 0.9703, 0.8481] 
2025-02-10 05:36:20.610744: Epoch time: 390.78 s 
2025-02-10 05:36:21.520239:  
2025-02-10 05:36:21.520608: Epoch 90 
2025-02-10 05:36:21.520711: Current learning rate: 0.00919 
2025-02-10 05:42:52.874819: train_loss -0.9189 
2025-02-10 05:42:52.878074: val_loss -0.8598 
2025-02-10 05:42:52.878203: Pseudo dice [0.9525, 0.9729, 0.8122] 
2025-02-10 05:42:52.879247: Epoch time: 391.35 s 
2025-02-10 05:42:53.780554:  
2025-02-10 05:42:53.780735: Epoch 91 
2025-02-10 05:42:53.780839: Current learning rate: 0.00918 
2025-02-10 05:49:23.648865: train_loss -0.9118 
2025-02-10 05:49:23.651861: val_loss -0.8777 
2025-02-10 05:49:23.652014: Pseudo dice [0.9526, 0.973, 0.8121] 
2025-02-10 05:49:23.652730: Epoch time: 389.87 s 
2025-02-10 05:49:24.526230:  
2025-02-10 05:49:24.526448: Epoch 92 
2025-02-10 05:49:24.526550: Current learning rate: 0.00917 
2025-02-10 05:55:54.485319: train_loss -0.9261 
2025-02-10 05:55:54.488045: val_loss -0.8634 
2025-02-10 05:55:54.488214: Pseudo dice [0.9375, 0.9732, 0.8129] 
2025-02-10 05:55:54.488951: Epoch time: 389.96 s 
2025-02-10 05:55:55.369352:  
2025-02-10 05:55:55.369559: Epoch 93 
2025-02-10 05:55:55.369651: Current learning rate: 0.00916 
2025-02-10 06:02:25.865955: train_loss -0.9293 
2025-02-10 06:02:25.868625: val_loss -0.8676 
2025-02-10 06:02:25.868788: Pseudo dice [0.938, 0.9742, 0.8597] 
2025-02-10 06:02:25.869491: Epoch time: 390.5 s 
2025-02-10 06:02:26.720530:  
2025-02-10 06:02:26.720724: Epoch 94 
2025-02-10 06:02:26.720817: Current learning rate: 0.00915 
2025-02-10 06:08:57.295864: train_loss -0.9366 
2025-02-10 06:08:57.298566: val_loss -0.8831 
2025-02-10 06:08:57.298692: Pseudo dice [0.9386, 0.9753, 0.8585] 
2025-02-10 06:08:57.299600: Epoch time: 390.58 s 
2025-02-10 06:08:59.159588:  
2025-02-10 06:08:59.159772: Epoch 95 
2025-02-10 06:08:59.159864: Current learning rate: 0.00914 
2025-02-10 06:15:28.870732: train_loss -0.937 
2025-02-10 06:15:28.873256: val_loss -0.8663 
2025-02-10 06:15:28.873462: Pseudo dice [0.935, 0.9727, 0.832] 
2025-02-10 06:15:28.874215: Epoch time: 389.71 s 
2025-02-10 06:15:29.753541:  
2025-02-10 06:15:29.753735: Epoch 96 
2025-02-10 06:15:29.753842: Current learning rate: 0.00913 
2025-02-10 06:21:59.775337: train_loss -0.9368 
2025-02-10 06:21:59.778169: val_loss -0.8692 
2025-02-10 06:21:59.778304: Pseudo dice [0.9438, 0.9741, 0.8331] 
2025-02-10 06:21:59.778980: Epoch time: 390.02 s 
2025-02-10 06:22:00.639830:  
2025-02-10 06:22:00.640018: Epoch 97 
2025-02-10 06:22:00.640116: Current learning rate: 0.00912 
2025-02-10 06:28:30.609104: train_loss -0.9347 
2025-02-10 06:28:30.612232: val_loss -0.882 
2025-02-10 06:28:30.612406: Pseudo dice [0.9395, 0.9733, 0.8481] 
2025-02-10 06:28:30.613162: Epoch time: 389.97 s 
2025-02-10 06:28:31.487500:  
2025-02-10 06:28:31.487685: Epoch 98 
2025-02-10 06:28:31.487783: Current learning rate: 0.00911 
2025-02-10 06:35:01.276227: train_loss -0.9324 
2025-02-10 06:35:01.279002: val_loss -0.8851 
2025-02-10 06:35:01.279189: Pseudo dice [0.9374, 0.9756, 0.8469] 
2025-02-10 06:35:01.279960: Epoch time: 389.79 s 
2025-02-10 06:35:02.182049:  
2025-02-10 06:35:02.182249: Epoch 99 
2025-02-10 06:35:02.182359: Current learning rate: 0.0091 
2025-02-10 06:41:31.334994: train_loss -0.9377 
2025-02-10 06:41:31.337167: val_loss -0.869 
2025-02-10 06:41:31.337342: Pseudo dice [0.9324, 0.9741, 0.8627] 
2025-02-10 06:41:31.338060: Epoch time: 389.15 s 
2025-02-10 06:41:32.593321:  
2025-02-10 06:41:32.593675: Epoch 100 
2025-02-10 06:41:32.593786: Current learning rate: 0.0091 
2025-02-10 06:48:03.693211: train_loss -0.9305 
2025-02-10 06:48:03.695926: val_loss -0.8818 
2025-02-10 06:48:03.696225: Pseudo dice [0.9271, 0.9749, 0.8691] 
2025-02-10 06:48:03.697579: Epoch time: 391.1 s 
2025-02-10 06:48:04.615865:  
2025-02-10 06:48:04.616101: Epoch 101 
2025-02-10 06:48:04.616199: Current learning rate: 0.00909 
2025-02-10 06:54:35.585776: train_loss -0.9353 
2025-02-10 06:54:35.589738: val_loss -0.8768 
2025-02-10 06:54:35.590379: Pseudo dice [0.9382, 0.9748, 0.8565] 
2025-02-10 06:54:35.591446: Epoch time: 390.97 s 
2025-02-10 06:54:36.515251:  
2025-02-10 06:54:36.515482: Epoch 102 
2025-02-10 06:54:36.515588: Current learning rate: 0.00908 
2025-02-10 07:01:07.216716: train_loss -0.94 
2025-02-10 07:01:07.218836: val_loss -0.8986 
2025-02-10 07:01:07.218991: Pseudo dice [0.9357, 0.9744, 0.8998] 
2025-02-10 07:01:07.219741: Epoch time: 390.7 s 
2025-02-10 07:01:07.219969: Yayy! New best EMA pseudo Dice: 0.9211 
2025-02-10 07:01:08.459964:  
2025-02-10 07:01:08.460372: Epoch 103 
2025-02-10 07:01:08.460493: Current learning rate: 0.00907 
2025-02-10 07:07:39.927326: train_loss -0.9417 
2025-02-10 07:07:39.930621: val_loss -0.8883 
2025-02-10 07:07:39.930779: Pseudo dice [0.9334, 0.9765, 0.8813] 
2025-02-10 07:07:39.931458: Epoch time: 391.47 s 
2025-02-10 07:07:39.931554: Yayy! New best EMA pseudo Dice: 0.9221 
2025-02-10 07:07:42.178487:  
2025-02-10 07:07:42.178811: Epoch 104 
2025-02-10 07:07:42.178923: Current learning rate: 0.00906 
2025-02-10 07:14:12.567520: train_loss -0.9382 
2025-02-10 07:14:12.570405: val_loss -0.8754 
2025-02-10 07:14:12.570526: Pseudo dice [0.939, 0.9745, 0.8392] 
2025-02-10 07:14:12.571382: Epoch time: 390.39 s 
2025-02-10 07:14:13.459138:  
2025-02-10 07:14:13.459347: Epoch 105 
2025-02-10 07:14:13.459450: Current learning rate: 0.00905 
2025-02-10 07:20:42.229383: train_loss -0.9394 
2025-02-10 07:20:42.232434: val_loss -0.8982 
2025-02-10 07:20:42.232598: Pseudo dice [0.9415, 0.9762, 0.8739] 
2025-02-10 07:20:42.233342: Epoch time: 388.77 s 
2025-02-10 07:20:42.233531: Yayy! New best EMA pseudo Dice: 0.9225 
2025-02-10 07:20:43.543977:  
2025-02-10 07:20:43.544355: Epoch 106 
2025-02-10 07:20:43.544592: Current learning rate: 0.00904 
2025-02-10 07:27:14.189421: train_loss -0.9329 
2025-02-10 07:27:14.192630: val_loss -0.8823 
2025-02-10 07:27:14.192751: Pseudo dice [0.9371, 0.9746, 0.8477] 
2025-02-10 07:27:14.193374: Epoch time: 390.65 s 
2025-02-10 07:27:15.058510:  
2025-02-10 07:27:15.058716: Epoch 107 
2025-02-10 07:27:15.058810: Current learning rate: 0.00903 
2025-02-10 07:33:45.591863: train_loss -0.9408 
2025-02-10 07:33:45.594729: val_loss -0.8765 
2025-02-10 07:33:45.594878: Pseudo dice [0.938, 0.9744, 0.8418] 
2025-02-10 07:33:45.595591: Epoch time: 390.53 s 
2025-02-10 07:33:46.478886:  
2025-02-10 07:33:46.479109: Epoch 108 
2025-02-10 07:33:46.479205: Current learning rate: 0.00902 
2025-02-10 07:40:15.153462: train_loss -0.9414 
2025-02-10 07:40:15.156176: val_loss -0.8737 
2025-02-10 07:40:15.156353: Pseudo dice [0.9377, 0.9749, 0.8537] 
2025-02-10 07:40:15.157154: Epoch time: 388.68 s 
2025-02-10 07:40:16.031343:  
2025-02-10 07:40:16.031573: Epoch 109 
2025-02-10 07:40:16.031675: Current learning rate: 0.00901 
2025-02-10 07:46:46.796476: train_loss -0.9349 
2025-02-10 07:46:46.799605: val_loss -0.8727 
2025-02-10 07:46:46.799888: Pseudo dice [0.9323, 0.9733, 0.8335] 
2025-02-10 07:46:46.801204: Epoch time: 390.76 s 
2025-02-10 07:46:47.729159:  
2025-02-10 07:46:47.729381: Epoch 110 
2025-02-10 07:46:47.729531: Current learning rate: 0.009 
2025-02-10 07:53:19.553413: train_loss -0.9356 
2025-02-10 07:53:19.556428: val_loss -0.8849 
2025-02-10 07:53:19.556788: Pseudo dice [0.9412, 0.9745, 0.8578] 
2025-02-10 07:53:19.557502: Epoch time: 391.82 s 
2025-02-10 07:53:20.551779:  
2025-02-10 07:53:20.551987: Epoch 111 
2025-02-10 07:53:20.552097: Current learning rate: 0.009 
2025-02-10 07:59:50.887142: train_loss -0.9352 
2025-02-10 07:59:50.889819: val_loss -0.883 
2025-02-10 07:59:50.890198: Pseudo dice [0.9373, 0.9693, 0.8557] 
2025-02-10 07:59:50.890998: Epoch time: 390.34 s 
2025-02-10 07:59:51.812085:  
2025-02-10 07:59:51.812481: Epoch 112 
2025-02-10 07:59:51.812585: Current learning rate: 0.00899 
2025-02-10 08:06:22.292148: train_loss -0.9208 
2025-02-10 08:06:22.294685: val_loss -0.89 
2025-02-10 08:06:22.294839: Pseudo dice [0.9465, 0.9715, 0.8634] 
2025-02-10 08:06:22.295523: Epoch time: 390.48 s 
2025-02-10 08:06:24.144321:  
2025-02-10 08:06:24.144482: Epoch 113 
2025-02-10 08:06:24.144584: Current learning rate: 0.00898 
2025-02-10 08:12:54.926408: train_loss -0.9274 
2025-02-10 08:12:54.929297: val_loss -0.8929 
2025-02-10 08:12:54.929503: Pseudo dice [0.9391, 0.9758, 0.8797] 
2025-02-10 08:12:54.931810: Epoch time: 390.78 s 
2025-02-10 08:12:54.931935: Yayy! New best EMA pseudo Dice: 0.9228 
2025-02-10 08:12:56.192665:  
2025-02-10 08:12:56.193115: Epoch 114 
2025-02-10 08:12:56.193250: Current learning rate: 0.00897 
2025-02-10 08:19:26.394487: train_loss -0.933 
2025-02-10 08:19:26.397566: val_loss -0.8627 
2025-02-10 08:19:26.397761: Pseudo dice [0.9293, 0.9726, 0.841] 
2025-02-10 08:19:26.398544: Epoch time: 390.2 s 
2025-02-10 08:19:27.251577:  
2025-02-10 08:19:27.251759: Epoch 115 
2025-02-10 08:19:27.251858: Current learning rate: 0.00896 
2025-02-10 08:25:57.750751: train_loss -0.9328 
2025-02-10 08:25:57.754175: val_loss -0.8424 
2025-02-10 08:25:57.754458: Pseudo dice [0.9346, 0.9726, 0.8001] 
2025-02-10 08:25:57.756123: Epoch time: 390.5 s 
2025-02-10 08:25:58.674842:  
2025-02-10 08:25:58.675030: Epoch 116 
2025-02-10 08:25:58.675137: Current learning rate: 0.00895 
2025-02-10 08:32:29.686696: train_loss -0.9268 
2025-02-10 08:32:29.689417: val_loss -0.8574 
2025-02-10 08:32:29.689572: Pseudo dice [0.9264, 0.9751, 0.8196] 
2025-02-10 08:32:29.690282: Epoch time: 391.01 s 
2025-02-10 08:32:30.595742:  
2025-02-10 08:32:30.595981: Epoch 117 
2025-02-10 08:32:30.596102: Current learning rate: 0.00894 
2025-02-10 08:38:59.694608: train_loss -0.9333 
2025-02-10 08:38:59.697454: val_loss -0.8856 
2025-02-10 08:38:59.697592: Pseudo dice [0.9364, 0.9753, 0.8621] 
2025-02-10 08:38:59.698279: Epoch time: 389.1 s 
2025-02-10 08:39:00.592773:  
2025-02-10 08:39:00.593026: Epoch 118 
2025-02-10 08:39:00.593170: Current learning rate: 0.00893 
2025-02-10 08:45:30.686437: train_loss -0.9289 
2025-02-10 08:45:30.689144: val_loss -0.8814 
2025-02-10 08:45:30.689329: Pseudo dice [0.9435, 0.9708, 0.8615] 
2025-02-10 08:45:30.690215: Epoch time: 390.09 s 
2025-02-10 08:45:31.560691:  
2025-02-10 08:45:31.560867: Epoch 119 
2025-02-10 08:45:31.560968: Current learning rate: 0.00892 
2025-02-10 08:52:00.918840: train_loss -0.9382 
2025-02-10 08:52:00.921572: val_loss -0.8726 
2025-02-10 08:52:00.921731: Pseudo dice [0.9338, 0.9748, 0.8606] 
2025-02-10 08:52:00.922437: Epoch time: 389.36 s 
2025-02-10 08:52:01.827253:  
2025-02-10 08:52:01.827478: Epoch 120 
2025-02-10 08:52:01.827589: Current learning rate: 0.00891 
2025-02-10 08:58:32.788862: train_loss -0.9349 
2025-02-10 08:58:32.791522: val_loss -0.8844 
2025-02-10 08:58:32.791635: Pseudo dice [0.9381, 0.9736, 0.8403] 
2025-02-10 08:58:32.792679: Epoch time: 390.96 s 
2025-02-10 08:58:33.721864:  
2025-02-10 08:58:33.722065: Epoch 121 
2025-02-10 08:58:33.722166: Current learning rate: 0.0089 
2025-02-10 09:05:04.299742: train_loss -0.9352 
2025-02-10 09:05:04.303445: val_loss -0.8833 
2025-02-10 09:05:04.303784: Pseudo dice [0.9409, 0.9762, 0.8555] 
2025-02-10 09:05:04.304743: Epoch time: 390.58 s 
2025-02-10 09:05:06.265466:  
2025-02-10 09:05:06.265711: Epoch 122 
2025-02-10 09:05:06.265816: Current learning rate: 0.00889 
2025-02-10 09:11:37.585188: train_loss -0.937 
2025-02-10 09:11:37.588091: val_loss -0.8773 
2025-02-10 09:11:37.588236: Pseudo dice [0.9359, 0.9732, 0.8533] 
2025-02-10 09:11:37.589320: Epoch time: 391.32 s 
2025-02-10 09:11:38.533479:  
2025-02-10 09:11:38.533686: Epoch 123 
2025-02-10 09:11:38.533793: Current learning rate: 0.00889 
2025-02-10 09:18:08.537035: train_loss -0.9356 
2025-02-10 09:18:08.539893: val_loss -0.8862 
2025-02-10 09:18:08.540087: Pseudo dice [0.9168, 0.9748, 0.8813] 
2025-02-10 09:18:08.540835: Epoch time: 390.0 s 
2025-02-10 09:18:09.444253:  
2025-02-10 09:18:09.444442: Epoch 124 
2025-02-10 09:18:09.444544: Current learning rate: 0.00888 
2025-02-10 09:24:40.381970: train_loss -0.941 
2025-02-10 09:24:40.384673: val_loss -0.8793 
2025-02-10 09:24:40.384809: Pseudo dice [0.9381, 0.9746, 0.862] 
2025-02-10 09:24:40.385476: Epoch time: 390.94 s 
2025-02-10 09:24:41.283803:  
2025-02-10 09:24:41.284019: Epoch 125 
2025-02-10 09:24:41.284134: Current learning rate: 0.00887 
2025-02-10 09:31:11.930608: train_loss -0.9458 
2025-02-10 09:31:11.933004: val_loss -0.8663 
2025-02-10 09:31:11.933298: Pseudo dice [0.9395, 0.9729, 0.835] 
2025-02-10 09:31:11.934092: Epoch time: 390.65 s 
2025-02-10 09:31:12.877121:  
2025-02-10 09:31:12.877323: Epoch 126 
2025-02-10 09:31:12.877468: Current learning rate: 0.00886 
2025-02-10 09:37:43.697447: train_loss -0.9459 
2025-02-10 09:37:43.700534: val_loss -0.8797 
2025-02-10 09:37:43.700693: Pseudo dice [0.9346, 0.9744, 0.8525] 
2025-02-10 09:37:43.701483: Epoch time: 390.82 s 
2025-02-10 09:37:44.575368:  
2025-02-10 09:37:44.575560: Epoch 127 
2025-02-10 09:37:44.575661: Current learning rate: 0.00885 
2025-02-10 09:44:13.340301: train_loss -0.9426 
2025-02-10 09:44:13.343096: val_loss -0.8775 
2025-02-10 09:44:13.343259: Pseudo dice [0.939, 0.9758, 0.8542] 
2025-02-10 09:44:13.344170: Epoch time: 388.77 s 
2025-02-10 09:44:14.214077:  
2025-02-10 09:44:14.214281: Epoch 128 
2025-02-10 09:44:14.214393: Current learning rate: 0.00884 
2025-02-10 09:50:45.077683: train_loss -0.9427 
2025-02-10 09:50:45.080008: val_loss -0.8821 
2025-02-10 09:50:45.080178: Pseudo dice [0.9351, 0.9739, 0.8465] 
2025-02-10 09:50:45.080935: Epoch time: 390.86 s 
2025-02-10 09:50:45.967332:  
2025-02-10 09:50:45.967562: Epoch 129 
2025-02-10 09:50:45.967673: Current learning rate: 0.00883 
2025-02-10 09:57:16.582261: train_loss -0.9448 
2025-02-10 09:57:16.585132: val_loss -0.8919 
2025-02-10 09:57:16.585332: Pseudo dice [0.9416, 0.9745, 0.8558] 
2025-02-10 09:57:16.586086: Epoch time: 390.62 s 
2025-02-10 09:57:18.430411:  
2025-02-10 09:57:18.430631: Epoch 130 
2025-02-10 09:57:18.430744: Current learning rate: 0.00882 
